{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd8b1b0-f924-487f-8c57-ceb8ad06b889",
   "metadata": {},
   "source": [
    "# Small demo notebook for misclassification detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314df5ba-4b6f-4468-aab5-9c1dbe670c4e",
   "metadata": {},
   "source": [
    "In this notebook we will train some models on MRPC dataset and explore several popular methods for uncertainty estimation (UE) - MC dropout, Mahalanobis distance and ensembles. So let's start from importing necessary functions and training models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d93855-b35c-461c-a0ac-1d7ec6d5a139",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84c41d-450c-4f03-ae22-9a0347293afb",
   "metadata": {},
   "source": [
    "We will train [ELECTRA](https://arxiv.org/abs/2003.10555) model for text classification. But firstly we will import necessary functions. Also don't forget to install dependencies - it's located in ../requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21ea026-7c8b-4813-a361-b330710c55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 16:07:29.601552: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from demo_utils import train_model, preproc_config, train_model_ensemble, get_table, eval_model, calc_mc_dropout\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import open_dict\n",
    "import hydra\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "log = logging.getLogger(__name__)\n",
    "# if use cuda\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073092f3-22bc-4765-aa61-0858ce6f8c5e",
   "metadata": {},
   "source": [
    "To simplify process of setting training parameters, we load predefined config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63da59ac-c47a-47c7-b810-14d1c678e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-ue-exps-0/lib/python3.7/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'mrpc': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Let's set model params with config\n",
    "# We will use MRPC dataset\n",
    "configs_dir = \"../configs\"\n",
    "config_path = \"mrpc\"\n",
    "abs_config_dir=os.path.abspath(configs_dir)\n",
    "with initialize_config_dir(config_dir=abs_config_dir):\n",
    "    config = compose(config_name=config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9f74d-6b77-41d4-be4a-d8af22854b90",
   "metadata": {},
   "source": [
    "Modify config a little before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65b8d8d-a59e-4bcc-9a75-6516d11c676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Set config only to train and train model\n",
    "config.do_train = True\n",
    "config.do_eval = True\n",
    "config.do_ue_estimate = False\n",
    "config.data.validation_subsample = 0.0\n",
    "# set dir for saving model and results\n",
    "with open_dict(config):\n",
    "    config.model_dir = '../workdir/model/'\n",
    "# preprocess config - update pathes for saving\n",
    "config, _, _ = preproc_config(config, init=True)\n",
    "# seeds\n",
    "seeds = [42, 4519, 941]\n",
    "# number of models in ensemble\n",
    "num_models = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b552f2-f807-4b5e-a6e0-e1f7ddd8bf70",
   "metadata": {},
   "source": [
    "Now we are ready for training models. We will train several models with different seeds, so we could obtain mean values for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9110abc2-0530-40c8-ab69-e06fe4a1b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    config.seed = seed\n",
    "    train_model(config)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d7164c-4bbd-4557-8557-6db5aade581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also train models for ensemble\n",
    "train_model_ensemble(config, seeds, num_models)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30d0f5-73e0-4268-8880-1533b992d134",
   "metadata": {},
   "source": [
    "## Calculating UE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94924aac-d7f2-4317-a094-13c14f17e336",
   "metadata": {},
   "source": [
    "By now, we've trained several models on the MRPC dataset. So now we look at some methods of uncertainty estimation - [MC dropout on all layers](https://proceedings.mlr.press/v48/gal16.html) (MC), [Mahalanobis distance](https://ojs.aaai.org/index.php/AAAI/article/view/17612) (MD) and [Deep ensemble](https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf18659-d117-47f8-a071-0685d87c7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have trained model, let's use it and get uncertainty estimation scores\n",
    "# Set config to UE\n",
    "config.do_train = False\n",
    "config.do_eval = True\n",
    "config.do_ue_estimate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8be4a6-a7ca-491b-9bab-10aaeae2caf4",
   "metadata": {},
   "source": [
    "At first, create main function for uncertainty estimation. In this function we simply create estimator and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7de5bc-0c8d-450e-b9bb-8bb4ff792c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ue_estimator(\n",
    "    model,\n",
    "    ue_args,\n",
    "    eval_metric,\n",
    "    calibration_dataset,\n",
    "    train_dataset,\n",
    "    cache_dir,\n",
    "    config=None,\n",
    "):\n",
    "    if ue_args.ue_type == \"mc\" or ue_args.ue_type == \"mc-dc\":\n",
    "        return UeEstimatorMc(\n",
    "            model, ue_args, eval_metric, calibration_dataset, train_dataset\n",
    "        )\n",
    "    elif ue_args.ue_type == \"maha\":\n",
    "        return UeEstimatorMahalanobis(model, ue_args, config, train_dataset)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "def estimate(\n",
    "    config,\n",
    "    classifier,\n",
    "    eval_metric,\n",
    "    calibration_dataset,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    eval_results,\n",
    "    work_dir\n",
    "):\n",
    "    \"\"\"Function for uncertainty estimation\"\"\"\n",
    "    true_labels = eval_results[\"true_labels\"]\n",
    "    # create estimator\n",
    "    ue_estimator = create_ue_estimator(\n",
    "        classifier,\n",
    "        config.ue,\n",
    "        eval_metric,\n",
    "        calibration_dataset=calibration_dataset,\n",
    "        train_dataset=train_dataset,\n",
    "        cache_dir=config.cache_dir,\n",
    "        config=config,\n",
    "    )\n",
    "    # calc UE\n",
    "    ue_results = ue_estimator(eval_dataset, true_labels)\n",
    "    # save results\n",
    "    eval_results.update(ue_results)\n",
    "    with open(Path(work_dir) / \"dev_inference.json\", \"w\") as res:\n",
    "        json.dump(eval_results, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85780d03-231e-490f-8ad7-c10ef17bc177",
   "metadata": {},
   "source": [
    "### MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731a93d-a306-4eaa-aaec-e75a268a1e2a",
   "metadata": {},
   "source": [
    "Now we implement estimators for each method. Let's start from MC dropout.\\\n",
    "Firstly, we write custom dropout layer and functions for replacing all model dropouts to our custom version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a43f5985-a8dd-442c-b70d-d8c218d5263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMC(torch.nn.Module):\n",
    "    def __init__(self, p: float, activate=False):\n",
    "        super().__init__()\n",
    "        self.activate = activate\n",
    "        self.p = p\n",
    "        self.p_init = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return torch.nn.functional.dropout(\n",
    "            x, self.p, training=self.training or self.activate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1261bbf1-ff27-4abf-9c4d-fb31194f5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dropouts(model, ue_args):\n",
    "    \"\"\"This function replace all model dropouts with custom dropout layer.\"\"\"\n",
    "    dropout_ctor = lambda p, activate: DropoutMC(\n",
    "        p=ue_args.inference_prob, activate=False\n",
    "    )\n",
    "    convert_to_mc_dropout(model, {\"Dropout\": dropout_ctor, \"StableDropout\": dropout_ctor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0adbe897-db34-4c4a-9b51-e9bc30fc2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_mc_dropout(\n",
    "    model, substitution_dict\n",
    "):\n",
    "    for i, layer in enumerate(list(model.children())):\n",
    "        proba_field_name = \"dropout_rate\" if \"flair\" in str(type(layer)) else \"p\"\n",
    "        module_name = list(model._modules.items())[i][0]\n",
    "        layer_name = layer._get_name()\n",
    "        proba_field_name = \"drop_prob\" if layer_name == \"StableDropout\" else proba_field_name #DeBERTA case\n",
    "        if layer_name in substitution_dict.keys():\n",
    "            model._modules[module_name] = substitution_dict[layer_name](\n",
    "                p=getattr(layer, proba_field_name), activate=False\n",
    "            )\n",
    "        else:\n",
    "            convert_to_mc_dropout(model=layer, substitution_dict=substitution_dict)\n",
    "\n",
    "\n",
    "def activate_mc_dropout(\n",
    "    model: torch.nn.Module, activate: bool, random: float = 0.0, verbose: bool = False\n",
    "):\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, DropoutMC):\n",
    "            if verbose:\n",
    "                print(layer)\n",
    "                print(f\"Current DO state: {layer.activate}\")\n",
    "                print(f\"Switching state to: {activate}\")\n",
    "            layer.activate = activate\n",
    "            if activate and random:\n",
    "                layer.p = random\n",
    "            if not activate:\n",
    "                layer.p = layer.p_init\n",
    "        else:\n",
    "            activate_mc_dropout(\n",
    "                model=layer, activate=activate, random=random, verbose=verbose\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44376a6-3422-41da-9c5f-e8f5191c808c",
   "metadata": {},
   "source": [
    "Now implement estimator for MC Dropout - in this class we simply replace all dropouts with custom layers, activate them, and calc model's predictions several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73aef112-879d-4c8a-8767-06ff5df51f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UeEstimatorMc:\n",
    "    def __init__(self, cls, ue_args, eval_metric, calibration_dataset, train_dataset):\n",
    "        self.cls = cls\n",
    "        self.ue_args = ue_args\n",
    "        self.calibration_dataset = calibration_dataset\n",
    "        self.eval_metric = eval_metric\n",
    "        self.train_dataset = train_dataset\n",
    "\n",
    "    def __call__(self, eval_dataset, true_labels=None):\n",
    "        ue_args = self.ue_args\n",
    "        eval_metric = self.eval_metric\n",
    "        model = self.cls._auto_model\n",
    "\n",
    "        log.info(\"******Perform stochastic inference...*******\")\n",
    "\n",
    "        #if ue_args.dropout_type == \"DC_MC\":\n",
    "        #    activate_mc_dropconnect(model, activate=True, random=ue_args.inference_prob)\n",
    "        #else:\n",
    "        convert_dropouts(model, ue_args)\n",
    "        activate_mc_dropout(model, activate=True, random=ue_args.inference_prob)\n",
    "\n",
    "        if ue_args.use_cache:\n",
    "            log.info(\"Caching enabled.\")\n",
    "            model.enable_cache()\n",
    "\n",
    "        eval_results = {}\n",
    "        eval_results[\"sampled_probabilities\"] = []\n",
    "        eval_results[\"sampled_answers\"] = []\n",
    "\n",
    "        log.info(\"****************Start runs**************\")\n",
    "\n",
    "        for i in tqdm(range(ue_args.committee_size)):\n",
    "            preds, probs = self.cls.predict(eval_dataset)[:2]\n",
    "\n",
    "            eval_results[\"sampled_probabilities\"].append(probs.tolist())\n",
    "            eval_results[\"sampled_answers\"].append(preds.tolist())\n",
    "\n",
    "            if ue_args.eval_passes:\n",
    "                eval_score = eval_metric.compute(\n",
    "                    predictions=preds, references=true_labels\n",
    "                )\n",
    "                log.info(f\"Eval score: {eval_score}\")\n",
    "\n",
    "        log.info(\"**************Done.********************\")\n",
    "\n",
    "        activate_mc_dropout(model, activate=False)\n",
    "\n",
    "        return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800652c-6760-4ee7-88ee-5122014b59c4",
   "metadata": {},
   "source": [
    "Now we only have to modify our config and add to it parameters for MC dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7733d0b0-b132-4a9c-bccf-810a0bf71d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, args_train, args_data = preproc_config(config)\n",
    "config.ue.ue_type = 'mc'\n",
    "config.ue.dropout_type = 'MC'\n",
    "config.ue.inference_prob = 0.1\n",
    "config.ue.committee_size = 20\n",
    "config.ue.dropout_subs = 'all'\n",
    "config.ue.use_cache = True\n",
    "config.ue.eval_passes = False\n",
    "config.ue.calibrate = False\n",
    "config.ue.use_selective = False\n",
    "for seed in seeds:\n",
    "    config.seed = seed\n",
    "    # eval model\n",
    "    classifier, eval_metric, calibration_dataset, train_dataset, eval_dataset, eval_results, work_dir = eval_model(config, 'mc', args_train, args_data)\n",
    "    # after calc UE for model\n",
    "    estimate(config, classifier, eval_metric, calibration_dataset, train_dataset, eval_dataset, eval_results, work_dir)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604129e8-6f8c-49d9-94f2-19d9e9769d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eaca87-57f3-42e2-94d6-3a83b42fde58",
   "metadata": {},
   "source": [
    "We will create estimator for Mahalanobis distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e5189-d80b-4c6d-80f8-562e47c1379e",
   "metadata": {},
   "source": [
    "Mahalanobis distance is a generalisation of the Euclidean distance, which takes into account the spreading of instances in the training set along various directions in a feature space. $u_{MD} = \\min_{c \\in C}(h_{i}-\\mu_{c})^{T}\\Sigma^{-1}(h_{i}-\\mu_{c}),$\\\n",
    "where $h_{i}$ is a hidden representation of a $i$-th instance, $\\mu_{c}$ is a centroid of a class $c$, and $\\Sigma$ is a covariance matrix for hidden representations of training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0a9845-1df6-4ad7-b74d-72d6682c57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a liitle modified ELECTRA head, we will use it for extracting model's features.\n",
    "class ElectraClassificationHeadIdentityPooler(torch.nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, other):\n",
    "        super().__init__()\n",
    "        self.dropout1 = other.dropout1\n",
    "        self.dense = other.dense\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ec643-2b6a-49a6-b0c0-0963ff63b22f",
   "metadata": {},
   "source": [
    "Functions for calculating mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48251f90-244b-4099-b213-b10f33d7fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(train_features, train_labels):\n",
    "    centroids = []\n",
    "    for label in np.sort(np.unique(train_labels)):\n",
    "        centroids.append(train_features[train_labels == label].mean(axis=0))\n",
    "    return np.asarray(centroids)\n",
    "\n",
    "\n",
    "def compute_covariance(centroids, train_features, train_labels):\n",
    "    cov = np.zeros((train_features.shape[1], train_features.shape[1]))\n",
    "    for c, mu_c in tqdm(enumerate(centroids)):\n",
    "        for x in train_features[train_labels == c]:\n",
    "            d = (x - mu_c)[:, None]\n",
    "            cov += d @ d.T\n",
    "    return cov / train_features.shape[0]\n",
    "\n",
    "\n",
    "def mahalanobis_distance(train_features, train_labels, eval_features):\n",
    "    centroids = compute_centroids(train_features, train_labels)\n",
    "    sigma = compute_covariance(centroids, train_features, train_labels)\n",
    "    diff = eval_features[:, None, :] - centroids[None, :, :]\n",
    "    try:\n",
    "        sigma_inv = np.linalg.inv(sigma)\n",
    "    except:\n",
    "        sigma_inv = np.linalg.pinv(sigma)\n",
    "        log.info(\"Compute pseudo-inverse matrix\")\n",
    "    dists = np.matmul(np.matmul(diff, sigma_inv), diff.transpose(0, 2, 1))\n",
    "    dists = np.asarray([np.diag(dist) for dist in dists])\n",
    "    return np.min(dists, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e235df3f-8ef9-460f-a02d-7c455bfdf5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UeEstimatorMahalanobis:\n",
    "    def __init__(self, cls, ue_args, config, train_dataset):\n",
    "        self.cls = cls\n",
    "        self.ue_args = ue_args\n",
    "        self.config = config\n",
    "        self.train_dataset = train_dataset\n",
    "\n",
    "    def __call__(self, eval_dataset, true_labels=None):\n",
    "        # because we use ELECTRA\n",
    "        self.cls.model.classifier = ElectraClassificationHeadIdentityPooler(\n",
    "            self.cls.model.classifier\n",
    "        )\n",
    "        log.info(\"Change classifier to Identity Pooler\")\n",
    "\n",
    "        eval_labels = [example[\"label\"] for example in eval_dataset]\n",
    "        eval_dataset = eval_dataset.remove_columns(\"label\")\n",
    "        eval_features = self.cls.predict(\n",
    "            eval_dataset, apply_softmax=False, return_preds=False\n",
    "        )[0]\n",
    "\n",
    "        train_labels = np.asarray([example[\"label\"] for example in self.train_dataset])\n",
    "        try:\n",
    "            self.train_dataset = self.train_dataset.remove_columns(\"label\")\n",
    "        except:\n",
    "            self.train_dataset.dataset = self.train_dataset.dataset.remove_columns(\n",
    "                \"label\"\n",
    "            )\n",
    "        train_features = self.cls.predict(\n",
    "            self.train_dataset, apply_softmax=False, return_preds=False\n",
    "        )[0]\n",
    "\n",
    "        eval_results = {}\n",
    "        eval_results[\"eval_labels\"] = true_labels\n",
    "        eval_results[\"mahalanobis_distance\"] = mahalanobis_distance(\n",
    "            train_features, train_labels, eval_features\n",
    "        ).tolist()\n",
    "\n",
    "        log.info(\"Done.\")\n",
    "\n",
    "        return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31b9d740-c396-4797-a49b-314c69754f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, args_train, args_data = preproc_config(config)\n",
    "config.ue.ue_type = 'maha'\n",
    "config.ue.dropout_type = ''\n",
    "config.ue.dropout_subs = ''\n",
    "config.ue.use_cache = True\n",
    "config.ue.eval_passes = False\n",
    "config.ue.calibrate = False\n",
    "config.ue.use_selective = False\n",
    "for seed in seeds:\n",
    "    config.seed = seed\n",
    "    # eval model\n",
    "    classifier, eval_metric, calibration_dataset, train_dataset, eval_dataset, eval_results, work_dir = eval_model(config, 'maha', args_train, args_data)\n",
    "    # after calc UE for model\n",
    "    estimate(config, classifier, eval_metric, calibration_dataset, train_dataset, eval_dataset, eval_results, work_dir)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e7a56-cfd4-473b-ac3d-766c9d40f8cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deep Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6644b5f-ad49-475e-9f1d-820cdc51fa93",
   "metadata": {},
   "source": [
    "The main idea of ensembles is quite simple - we just run several models and after stack results for each run. So here we don't need to write estimator, we will reuse eval results from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeddc4f5-735e-4982-b3cf-e8be18bbc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_results(results_dir, final_dir):\n",
    "    final_result = {\n",
    "        \"true_labels\": [],\n",
    "        \"probabilities\": [],\n",
    "        \"answers\": [],\n",
    "        \"sampled_probabilities\": [],\n",
    "        \"sampled_answers\": [],\n",
    "    }\n",
    "\n",
    "    for seed in os.listdir(results_dir):\n",
    "        results_file_path = Path(results_dir) / seed / \"dev_inference.json\"\n",
    "        with open(results_file_path) as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        final_result[\"sampled_probabilities\"].append(result[\"probabilities\"])\n",
    "        final_result[\"sampled_answers\"].append(result[\"answers\"])\n",
    "\n",
    "    final_result[\"true_labels\"] = result[\"true_labels\"]\n",
    "    final_result[\"answers\"] = result[\"answers\"]\n",
    "    final_result[\"probabilities\"] = result[\"probabilities\"]\n",
    "\n",
    "    with open(Path(final_dir) / \"dev_inference.json\", \"w\") as f:\n",
    "        json.dump(final_result, f)\n",
    "\n",
    "\n",
    "def calc_ensemble(config, seeds):\n",
    "    # here we simply stack model predictions\n",
    "    config, args_train, args_data = preproc_config(config)\n",
    "    # we assume that we have several trained and evaluated models\n",
    "    # so we just copy saved models output and stack it\n",
    "    # get dir with models by seeds\n",
    "    for idx in os.listdir(os.path.join(config.model_dir, 'ensemble')):\n",
    "        results_dir = os.path.join(config.model_dir, 'ensemble', idx)\n",
    "        ensemble_save_dir = os.path.join(config.output_dir, 'ensemble', idx)\n",
    "        os.makedirs(ensemble_save_dir, exist_ok=True)\n",
    "        accumulate_results(results_dir, ensemble_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0020e025-c9e9-4f17-bca3-af7a96b256f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# for ensemble we have a slightly different procedure - we already trained models for several seeds\n",
    "# so now we just reuse results from training\n",
    "calc_ensemble(config, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d097a-0b9d-42fc-bc9a-4c33d1dcb742",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f8370-1a0b-462f-adfe-6991528d1b3e",
   "metadata": {},
   "source": [
    "We already trained models and calculated uncertainty estimates, it's time to look at results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "735995bb-6074-4ed6-8319-c8e5ab8c3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_table(results, name, return_baseline=False):\n",
    "    \"\"\"Small helper function for building tables.\"\"\"\n",
    "    replace_dict = {'bald': 'BALD',\n",
    "                    'variance': 'PV',\n",
    "                    'sampled_max_prob': 'SMP',\n",
    "                    'mahalanobis_distance': 'MD',\n",
    "                    'sampled_entropy': 'SE',\n",
    "                    'var_ratio': 'VR'}\n",
    "    table = pd.DataFrame().from_dict(results)\n",
    "    scores = list(results[list(results.keys())[0]].index)\n",
    "    scores = [replace_dict.get(score, score) for score in scores]\n",
    "    table['UE Score'] = scores\n",
    "    table['Method'] = [name] * len(table)\n",
    "    table.drop('count', inplace=True)\n",
    "    baseline = table.loc['baseline (max_prob)']\n",
    "    baseline['Method'] = 'SR (baseline)'\n",
    "    baseline['UE Score'] = 'MP'\n",
    "    table.drop('baseline (max_prob)', inplace=True)\n",
    "    if return_baseline:\n",
    "        return table, baseline\n",
    "    else:\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5c95f-329a-4977-95a4-e828c30e1731",
   "metadata": {},
   "source": [
    "Here we use several different scores for uncertainty estimation - BALD, probability variance (PV), maximum probability (MP) and sampled maximum probability (SMP).\\\n",
    "The most simple score is MP:\\\n",
    "$u_{SR}(x) = 1 - \\max_{c \\in C} p(y=c|x).$\\\n",
    "As $C$ in this equation denoted number of classes, and $p(y|x)$ is probability over classes.\\\n",
    "SMP score formulated as following:\\\n",
    "$u_{SMP} = 1 -  \\max_{c \\in C} \\frac{1}{T}\\sum_{t=1}^T  p_t^{c}.$\\\n",
    "For this and for all next scores we assume that we have conducted $T$ stochastic passes. In this equation $p_t^{c}$ is the probability of the class $c$ for the $t$-th stochastic forward pass.\\\n",
    "PV score:\n",
    "$u_{PV} = \\frac{1}{C} \\sum_{c = 1}^C \\left( \\frac{1}{T - 1} \\sum_{t = 1}^T {(p^c_t-\\overline{p^c})^2} \\right),$\\\n",
    "where $\\overline{p^c}=\\frac{1}{T} \\sum_t{p^{c}_{t}}$ is the probability for a class $c$ averaged across $T$ stochastic forward passes.\\\n",
    "BALD score:\\\n",
    "$u_{B} = -\\sum_{c = 1}^C \\overline{p^c} \\log \\overline{p^c} +  \\frac{1}{T}\\sum_{c, t} p^{c}_{t}\\log p^{c}_{t}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac8d7956-445f-46e4-b80c-d1a403e33b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, let's set pathes to our results\n",
    "mc_path = os.path.join(config.output_dir, \"mc\")\n",
    "maha_path = os.path.join(config.output_dir, \"maha\")\n",
    "ensemble_path = os.path.join(config.output_dir, \"ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585af20-e344-46fa-ac0e-f5ccb47b4059",
   "metadata": {},
   "source": [
    "For evaluating quality of UE we will use reversed pair proportion [(RPP)](https://aclanthology.org/2021.acl-long.84/) and area under the risk-coverage curve [(RCC-AUC)](https://www.jmlr.org/papers/volume11/el-yaniv10a/el-yaniv10a.pdf) metrics.\\\n",
    "The risk coverage curve demonstrates the cumulative sum of loss due to misclassification (cumulative risk) depending on the uncertainty level used for rejection of predictions. The lower area under this curve indicates better quality of the UE method.\\\n",
    "RPP measures how far the uncertainty estimator $\\tilde{u}$ is to ideal, given the labeled dataset of size $n$:\n",
    "\n",
    "$RPP = \\frac{1}{n^2} \\displaystyle\\sum_{i,j = 1}^n \\mathbb{1}[\\tilde{u}(x_i) > \\tilde{u}(x_j), l_i > l_j].$\n",
    "\n",
    "For both metrics,  $l$ is an indicator loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84c65e10-8fac-4118-93e6-dbdea7016617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# We will calculate two metrics - RPP and RCC-AUC\n",
    "metrics = ['rpp', 'rcc-auc']\n",
    "mc_res = get_table(config, mc_path)\n",
    "maha_res = get_table(config, maha_path)\n",
    "ens_res = get_table(config, ensemble_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de40286a-6fdb-43ae-af1f-532d2afa3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_table, baseline = to_table(mc_res, 'MC', True)\n",
    "maha_table = to_table(maha_res, 'MD')\n",
    "ens_table = to_table(ens_res, 'Deep Ensemble')\n",
    "table = pd.concat([mc_table, maha_table, ens_table])\n",
    "table.loc['Baseline'] = baseline\n",
    "# Now make table more readable\n",
    "columns = ['Method', 'UE Score'] + metrics\n",
    "table = table[columns]\n",
    "table.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0bf8b22-1094-43d7-9a93-dbbc09543ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>rpp</th>\n",
       "      <th>rcc-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MC</td>\n",
       "      <td>BALD</td>\n",
       "      <td>2.27±0.31</td>\n",
       "      <td>18.31±3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MC</td>\n",
       "      <td>SMP</td>\n",
       "      <td>1.78±0.31</td>\n",
       "      <td>12.70±2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MC</td>\n",
       "      <td>PV</td>\n",
       "      <td>1.98±0.36</td>\n",
       "      <td>15.18±3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MD</td>\n",
       "      <td>MD</td>\n",
       "      <td>1.77±0.20</td>\n",
       "      <td>12.48±1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deep Ensemble</td>\n",
       "      <td>BALD</td>\n",
       "      <td>1.60±0.11</td>\n",
       "      <td>14.77±0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deep Ensemble</td>\n",
       "      <td>SMP</td>\n",
       "      <td>1.47±0.16</td>\n",
       "      <td>11.25±0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deep Ensemble</td>\n",
       "      <td>PV</td>\n",
       "      <td>1.50±0.19</td>\n",
       "      <td>13.20±2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SR (baseline)</td>\n",
       "      <td>MP</td>\n",
       "      <td>2.00±0.46</td>\n",
       "      <td>15.87±4.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Method UE Score        rpp     rcc-auc\n",
       "0             MC     BALD  2.27±0.31  18.31±3.67\n",
       "1             MC      SMP  1.78±0.31  12.70±2.09\n",
       "2             MC       PV  1.98±0.36  15.18±3.25\n",
       "3             MD       MD  1.77±0.20  12.48±1.38\n",
       "4  Deep Ensemble     BALD  1.60±0.11  14.77±0.94\n",
       "5  Deep Ensemble      SMP  1.47±0.16  11.25±0.33\n",
       "6  Deep Ensemble       PV  1.50±0.19  13.20±2.33\n",
       "7  SR (baseline)       MP  2.00±0.46  15.87±4.93"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c0e39-d6e5-44ae-b937-b09832281f56",
   "metadata": {},
   "source": [
    "As one can see, Deep Ensemble shows the best performance, but requires a lot of computational time. The second best method is Mahalanobis distance (MD), that shows good results and works faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73de94-f551-4580-91c3-e9c1f952b685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b7cd3-094f-4979-8d8b-f4e48b2fc5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
